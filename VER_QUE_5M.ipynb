{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a1a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import together\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Keys ===\n",
    "load_dotenv()\n",
    "together.api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "client = together.Together()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "claude_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "\n",
    "# MODEL_GENERATOR = \"gemini-2.5-flash\"\n",
    "MODEL_mx = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MODEL_gpt = \"gpt-4.1-2025-04-14\"\n",
    "MODEL_o3 = \"o3\"\n",
    "MODEL_cld = \"claude-3-5-sonnet-20241022\"\n",
    "MODEL_cld4 = \"claude-sonnet-4-20250514\"\n",
    "MODEL_gemini = \"gemini-2.5-pro\"\n",
    "MODEL_gemini_flash = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0609fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Utility: Call LLM ===\n",
    "# ---openai---\n",
    "def call_model_openai(prompt: str, model: str) -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "#---gemini---\n",
    "\n",
    "def call_model_gemini(prompt: str, model: str) -> str:\n",
    "    model = genai.GenerativeModel(model_name=model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "# ---together---\n",
    "\n",
    "def call_model_tog(prompt: str, model: str) -> str:\n",
    "    prompt = prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1.0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ---claude---\n",
    "\n",
    "def call_model_claude(prompt: str, model: str) -> str:\n",
    "    try:\n",
    "        response = claude_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"[Claude Error]\", e)\n",
    "        return \"[Error]\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2400cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Step 5: Get Final Answer from 2 Models ===\n",
    "def get_final_answer(question_json: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "You are a highly precise and expert-level science problem solver.\n",
    "\n",
    "Your task is to solve the following question and return **only the final boxed numeric result with proper SI units**, without showing any steps or reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### STRICT OUTPUT RULES:\n",
    "- Do NOT show any working, steps, or explanations.\n",
    "- Do NOT return anything other than the **final boxed answer**.\n",
    "- Use correct scientific symbols and SI units (e.g., `mol`, `kg·m/s²`, `J/K`, `g/L`, `nm`, `m/s`).\n",
    "- If symbolic constants (e.g., `π`, `ln(2)`, `e`, `R`) are involved, evaluate numerically to **at least 3 significant digits**.\n",
    "- If the question has no answer or is malformed, return exactly: `\"ERROR: Unsolvable or incomplete input.\"`\n",
    "\n",
    "---\n",
    "\n",
    "### SCIENCE QUESTION INPUT:\n",
    "{question_json}\n",
    "\n",
    "---\n",
    "\n",
    "### FINAL OUTPUT:\n",
    "**Strictly** Return just a single string with the *boxed numeric result*. *No markdown*, *no JSON*.\n",
    "→ `\"v = 2.67 m/s\"`  \n",
    "→ `\"ΔH = 35.8 kJ/mol\"`  \n",
    "→ `\"F = 9.81 N\"`  \n",
    "→ `\"Rate = 0.23 mol/L·s\"`  \n",
    "→ `\"ERROR: Unsolvable or incomplete input.\"`\n",
    "\"\"\"\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            \"gpt_o3\": executor.submit(call_model_openai, prompt, MODEL_o3),\n",
    "            \"gpt_4_1\": executor.submit(call_model_openai, prompt, MODEL_gpt),\n",
    "            \"claude_sonnet_4\": executor.submit(call_model_claude, prompt, MODEL_cld4),\n",
    "            \"gemini_2_5_pro\": executor.submit(call_model_gemini, prompt, MODEL_gemini),\n",
    "            \"gemini_flash\": executor.submit(call_model_gemini, prompt, MODEL_gemini_flash),\n",
    "        }\n",
    "        \n",
    "        return {k: f.result() for k, f in futures.items()}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12295f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Step 6: Compare Answers ===\n",
    "def compare_answers(ans1: str, ans2: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "You are a high-accuracy **scientific answer comparison engine**. Your role is to compare two final answers generated by different models for **science-based questions** (Physics, Chemistry, or Biology).\n",
    "\n",
    "Your task is to evaluate whether both answers express the **same scientific result**, accounting for:\n",
    "- Unit compatibility\n",
    "- Numerical closeness\n",
    "- Symbolic or structural equivalence\n",
    "- Scientific consistency\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Rubric\n",
    "\n",
    "Each of the following contributes to a **similarity score** between 0.0 and 1.0 (total 100%). The final decision is based on the score.\n",
    "\n",
    "| Criterion                    | Weight | Description |\n",
    "|-----------------------------|--------|-------------|\n",
    "| 1. Unit or Label Match      | 0.25   | Full score if units are the same or dimensionally equivalent (e.g., kJ/mol ≈ kcal/mol, N·m vs J, g vs mg). Applies to biological labels too (e.g., 'allele frequency', 'enzyme activity'). |\n",
    "| 2. Numerical Closeness      | 0.30   | Based on relative error. Full score if relative error < 1%, partial if between 1–2%, zero if >2%. |\n",
    "| 3. Symbolic/Expression Match| 0.15   | Covers symbolic equivalence, constants (π ≈ 3.14), or known scientific expressions (e.g., `ln(2)` ≈ 0.693, `RT` vs `8.314T`). |\n",
    "| 4. Rounding & Notation      | 0.10   | Accept differences in decimal points, scientific notation, or minor formatting. |\n",
    "| 5. Structural/Scientific Equivalence | 0.20 | Applies to alternate representations that are scientifically identical (e.g., `ΔH = +25 kJ/mol` vs `Enthalpy change = 25 kJ/mol`). |\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Decision Rule\n",
    "\n",
    "- If total score **≥ 0.80**, return: `\"decision\": \"similar\"`\n",
    "- If total score **< 0.80**, return: `\"decision\": \"different\"`\n",
    "\n",
    "---\n",
    "\n",
    "###  Example:\n",
    "\n",
    "**Answer 1:** `1/√2 m/s`  \n",
    "**Answer 2:** `0.707 m/s`\n",
    "Evaluation:\n",
    "\n",
    "- Units: m/s = m/s → 0.25\n",
    "- Numeric: 1/√2 ≈ 0.707 → 0.30\n",
    "- Symbolic/Decimal: equivalent → 0.15\n",
    "- Rounding: acceptable → 0.10\n",
    "- Expression: scalar match → 0.20\n",
    "\n",
    "**Total score = 1.00 → \"similar\"**\n",
    "\n",
    "**Answer 1:** `ΔG = -45.2 kJ/mol`  \n",
    "**Answer 2:** `Gibbs free energy = -45.2 kJ/mol`  \n",
    "→ Score = 1.00 → \"similar\"\n",
    "\n",
    "**Answer 1:** `C₆H₁₂O₆ + 6O₂ → 6CO₂ + 6H₂O`  \n",
    "**Answer 2:** `glucose reacts with oxygen to form carbon dioxide and water`  \n",
    "→ Score = ~0.85 → \"similar\"\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Compare the following two answers and return a **valid JSON object** in this format:\n",
    "\n",
    "{{\n",
    "  \"similarity_score\": <float between 0.0 and 1.0>,\n",
    "  \"decision\": \"similar\" or \"different\",\n",
    "  \"comment\": \"<brief technical explanation>\"\n",
    "}}\n",
    "\n",
    "Only return the JSON object. Do NOT include Markdown, LaTeX formatting, or extra commentary.\n",
    "### Input:\n",
    "Answer 1: {ans1}  \n",
    "Answer 2: {ans2}\n",
    "\"\"\"\n",
    "    response = call_model_openai(prompt, MODEL_gpt)\n",
    "    return json.loads(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ef8fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP: Compare all model answers ===\n",
    "def compare_all_model_answers(answer_dict: dict) -> list:\n",
    "    comparison_results = []\n",
    "    for (name1, ans1), (name2, ans2) in combinations(answer_dict.items(), 2):\n",
    "        result = compare_answers(ans1, ans2)\n",
    "        result.update({\"model_1\": name1, \"model_2\": name2})\n",
    "        comparison_results.append(result)\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffc8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate agreement and label difficulty ===\n",
    "def evaluate_difficulty(comparison_results: list, answer_dict: dict) -> dict:\n",
    "    total = len(comparison_results)\n",
    "    similar = sum(1 for r in comparison_results if r[\"decision\"] == \"similar\")\n",
    "    avg_score = sum(r[\"similarity_score\"] for r in comparison_results) / total\n",
    "    disagreement_ratio = 1 - (similar / total)\n",
    "    unique_answers = len(set(answer_dict.values()))\n",
    "\n",
    "    if disagreement_ratio > 0.8:\n",
    "        label = \"hard\"\n",
    "    elif disagreement_ratio > 0.5:\n",
    "        label = \"medium\"\n",
    "    else:\n",
    "        label = \"easy\"\n",
    "\n",
    "    return {\n",
    "        \"agreement_ratio\": round(similar / total, 2),\n",
    "        \"average_similarity\": round(avg_score, 3),\n",
    "        \"disagreement_ratio\": round(disagreement_ratio, 2),\n",
    "        \"difficulty_label\": label,\n",
    "        \"unique_answer_count\": unique_answers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c17a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN PIPELINE ===\n",
    "def process_question_row(question_text):\n",
    "    try:\n",
    "        answer_dict = get_final_answer(question_text)\n",
    "        comparison_results = compare_all_model_answers(answer_dict)\n",
    "        difficulty_info = evaluate_difficulty(comparison_results, answer_dict)\n",
    "\n",
    "        row_data = {\n",
    "            \"gpt_o3_answer\": answer_dict.get(\"gpt_o3\"),\n",
    "            \"gpt_4_1_answer\": answer_dict.get(\"gpt_4_1\"),\n",
    "            \"claude_sonnet_4_answer\": answer_dict.get(\"claude_sonnet_4\"),\n",
    "            \"gemini_2_5_pro_answer\": answer_dict.get(\"gemini_2_5_pro\"),\n",
    "            \"gemini_flash_answer\": answer_dict.get(\"gemini_flash\"),\n",
    "            \"disagreement_ratio\": difficulty_info[\"disagreement_ratio\"],\n",
    "            \"agreement_ratio\": difficulty_info[\"agreement_ratio\"],\n",
    "            \"difficulty_label\": difficulty_info[\"difficulty_label\"],\n",
    "            \"unique_answer_count\": difficulty_info[\"unique_answer_count\"],\n",
    "            \"comparison_json\": json.dumps(comparison_results)\n",
    "        }\n",
    "        return row_data\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"gpt_o3_answer\": \"error\",\n",
    "            \"gpt_4_1_answer\": \"error\",\n",
    "            \"claude_sonnet_4_answer\": \"error\",\n",
    "            \"gemini_2_5_pro_answer\": \"error\",\n",
    "            \"gemini_flash_answer\": \"error\",\n",
    "            \"disagreement_ratio\": \"error\",\n",
    "            \"agreement_ratio\": \"error\",\n",
    "            \"difficulty_label\": \"error\",\n",
    "            \"unique_answer_count\": \"error\",\n",
    "            \"comparison_json\": str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e633a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === File Runner ===\n",
    "def run_verification_pipeline(input_csv_path, output_csv_path):\n",
    "    df = pd.read_csv(input_csv_path, encoding='utf-8')\n",
    "\n",
    "    print(f\"Processing {len(df)} questions...\")\n",
    "    results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Processing Q{idx+1}\")\n",
    "        row_data = process_question_row(row[\"question\"])\n",
    "        results.append({**row, **row_data})\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\" Saved results to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be0d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 37 questions...\n",
      "Processing Q1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Q2\n",
      "Processing Q3\n",
      "Processing Q4\n",
      "Processing Q5\n",
      "Processing Q6\n",
      "Processing Q7\n",
      "Processing Q8\n",
      "Processing Q9\n",
      "Processing Q10\n",
      "Processing Q11\n",
      "Processing Q12\n",
      "Processing Q13\n",
      "Processing Q14\n",
      "Processing Q15\n",
      "Processing Q16\n",
      "Processing Q17\n",
      "Processing Q18\n",
      "Processing Q19\n",
      "Processing Q20\n",
      "Processing Q21\n",
      "Processing Q22\n",
      "Processing Q23\n",
      "Processing Q24\n",
      "Processing Q25\n",
      "Processing Q26\n",
      "Processing Q27\n",
      "Processing Q28\n",
      "Processing Q29\n",
      "Processing Q30\n",
      "Processing Q31\n",
      "Processing Q32\n",
      "Processing Q33\n",
      "Processing Q34\n",
      "Processing Q35\n",
      "Processing Q36\n",
      "Processing Q37\n",
      " Saved results to testing_phds_output.csv\n"
     ]
    }
   ],
   "source": [
    "run_verification_pipeline(\"testing.csv\", \"testing_phds_output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5af3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
